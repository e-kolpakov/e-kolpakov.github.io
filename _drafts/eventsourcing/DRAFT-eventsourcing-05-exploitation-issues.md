---
layout: post
title: "TBD: eventsourcing exploitation issues"
tags: ["design principles", eventsourcing-series-2020]
image_link_base: /assets/img/DRAFT-eventsourcing
series_sequence_nr: 5
---

TBD

[Back to Table of Contents]({% link design/eventsourcing-series.md %}#table-of-contents)

# All the things that went wrong after the launch
 
## Wednesday Cassandra crush

Around three moths after the launch. Mystery - every week at 8am Wednesday, Cassandra (used as persistence store) would
crash, fail the persistence write, crash the persistence plugin and the whole system stopped working - until manual
restart.

How did we found out - system health monitoring.

What was the root cause - (TBD: I don't recall exactly, please halp). Triggering event was something to do with Cassy 
stop-the-world GC[^2]. rott cause was some other service that were using Cassy as persistence store for akka actors,
but used Akka Singleton instead. Singleton accumulated huuuuuuge state and that was what was causing the GC to fire
more frequently, and take more time.

[^2]: at that moment devops suggested that-cassandra-implementation-in-C, but it never happened - probably neither us 
nor them were brave/crazy enough to actually go for it.

How did we solve it: it's a shame, but we just shut down that other service. At that point it was generally understood
that it's function was not necessary - due to changed business constraints. However, this decision returned to bite us
all - but that's probably a topic for a separate blog post.

## Wednesday Cassandra crush, again

... and not on Wednesday, but otherwise it was the same problem, actually.

How did we found out - the same way as previous one. 

Root cause: same - long GC pauses in Cassy. This time it was an application that pioneered akka and akka-persistence 
in the organization. Unfortunately, it haven't received much care since then - original authors left, and new owners
practiced "not broken - don't touch" a little bit too much. As a result, it used a very outdated version of cassandra
plugin, that used Cassy materialized views to implement certain features. That application received high traffic - it 
collected GPS tracks for all the vehicles, causing frequent updates to materialized views.

That Cassandra issue -- is actually caused by TOPS activity service which was crazily dumping JSON serialized events 
into C* store in some way it's not supposed to. So that the C* cluster who has maybe in total 96GB of memory at that 
time, was hosting like more than 150GB of data and tops-activity-service was trying to read very ancient data from 
time to time as it wants to recover without enough snapshot. That should have caused a lot of GC from time to time 
and caused the more time-sensitive TCS to fail in persisting events.

How did we solve it: shame again, but again we shut it down. Initially it was done as an emergency response, assuming
that current owners would update the app and turn it on again; unfortunately the "turn on" part never happened, AFAIK.

## Schema evolution